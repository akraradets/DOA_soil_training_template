{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipenv install opencv-python\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "from enum import Enum\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "class Devices(Enum):\n",
    "    iphone_11 = 'iphone 11'\n",
    "    samsung_s21 = 'samsung S21'\n",
    "    all = '*'\n",
    "\n",
    "class Environments(Enum):\n",
    "    indoor = 'Indoor'\n",
    "    outdoor = 'Outdoor'\n",
    "    all = '*'\n",
    "\n",
    "class Elements(Enum):\n",
    "    om = 'OM'\n",
    "    p = 'P'\n",
    "    k = 'K'\n",
    "\n",
    "class Preprocessing(Enum):\n",
    "    training = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(350),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    inferencing  = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(350),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "\n",
    "class SoilDataset_bigset(Dataset):\n",
    "    def __init__(self, dataset_name:str, element:Elements, device:Devices, \n",
    "                 environment:Environments, \n",
    "                 preprocessing:Preprocessing, \n",
    "                 clip_target:bool=False, \n",
    "                 normalize_target:bool=False):\n",
    "        self.element:Elements = element\n",
    "        self.clip_target:bool = clip_target\n",
    "        self.normalize_target:bool = normalize_target\n",
    "        \n",
    "        # Set max value for clipping and normalizing\n",
    "        self.max_value:float = 0\n",
    "        if(self.element == Elements.om):\n",
    "            self.max_value = 8.0\n",
    "        elif(self.element == Elements.p):\n",
    "            self.max_value = 1000.0\n",
    "        elif(self.element == Elements.k):\n",
    "            self.max_value = 1500.0\n",
    "\n",
    "        # BasePath of the dataset\n",
    "        base_path:str = \"../dataset\"\n",
    "        dataset_path:str = os.path.join(base_path, dataset_name)\n",
    "        if(os.path.exists(dataset_path) == False):\n",
    "            raise ValueError(f\"Path={dataset_path} is not exist. Execution path={os.getcwd()}\")\n",
    "\n",
    "        # Inside this path there must be a list of folders arange by mobile phone. Use device enum.\n",
    "        # Inside those mobile phone are 2 folders indicate the environment the image was taken in. Use environment enum.\n",
    "        image_folder = os.path.join(dataset_path, element.value, device.value, environment.value)\n",
    "        self.imgs = glob(os.path.join(image_folder,'*/*'))\n",
    "        print(f\"Found {len(self.imgs)} images in {image_folder}.\")\n",
    "\n",
    "        # Load csv file for lookup the target value\n",
    "        target_path:str = os.path.join(dataset_path,element.value,'meta.csv')\n",
    "        self.target_df = pd.read_csv(target_path, index_col='id')\n",
    "\n",
    "        self.signature = os.path.join(element.value,device.value,environment.value)\n",
    "        self.preprocessing = preprocessing.value\n",
    "\n",
    "    def get_target(self, img_path:str) -> float:\n",
    "        assert len(img_path.split('/')) == 8, f\"Expect img_path to have 8 folders but got {img_path=}\"\n",
    "        target_id = int(img_path.split('/')[6])\n",
    "        target_value = float(self.target_df.loc[target_id].iloc[0]) # type:ignore\n",
    "        if(self.clip_target and (target_value > self.max_value)):\n",
    "            target_value = self.max_value\n",
    "        if(self.normalize_target):\n",
    "            target_value = target_value / (self.max_value)\n",
    "        return float(target_value)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        y = self.get_target(img_path=img_path)\n",
    "        y = torch.tensor(y)\n",
    "        X = io.read_image(img_path)\n",
    "        if self.preprocessing:\n",
    "            X = self.preprocessing(X)\n",
    "        return X.float(), y.float(), img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images in ../dataset/workshop/OM/*/Outdoor.\n"
     ]
    }
   ],
   "source": [
    "dataset = SoilDataset_bigset(dataset_name=\"workshop\", \n",
    "                             element=Elements.om, \n",
    "                             device=Devices.all, \n",
    "                             environment=Environments.outdoor, \n",
    "                             preprocessing=Preprocessing.inferencing, \n",
    "                             clip_target=True, normalize_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "model = models.alexnet(weights=AlexNet_Weights.DEFAULT, progress=True)\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=1, bias=True)\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img:Image.Image, label:str, predict:str, picname:str):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()\n",
    "    # font.\n",
    "    import cv2\n",
    "    font = ImageFont.load_default(size=100)\n",
    "    draw.text((10, 10),f\"Label:{label}\",(0,0,0), font=font)\n",
    "    draw.text((10, 40 + font_size),f\"Predict:{predict}\",(0,0,0), font=font)\n",
    "    img.show(picname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,pic_name in dataset:\n",
    "    # print(pic_name)\n",
    "    yhat = model(x.reshape((1,3,224,224))).detach()[0][0]\n",
    "    img = Image.open(f'{pic_name}')\n",
    "    pic_name = os.path.join(pic_name)\n",
    "    show_image(img, y.numpy(), yhat.numpy(), pic_name)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
